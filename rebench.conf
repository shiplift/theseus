# -*- mode: yaml; -*-
# Config file for ReBench
# Config format is YAML (see http://yaml.org/ for detailed spec)

# this run definition will be choosen if no parameters are given to rebench.py
standard_run: Lamb
standard_data_file: 'results/lamb.data'

# settings and requirements for statistic evaluation
statistics:
  min_runs: 10
  max_runs: 10
  confidence_level: 0.5
  error_margin: 0.5
  #confidence_level: 0.95
  #error_margin: 0.005

# settings for quick runs, useful for fast feedback during experiments
quick_runs:
  min_runs: 3
  max_runs: 3
  max_time: 60   # time in seconds

# definition of benchmark suites
# settings in the benchmark suite will override similar settings of the VM
benchmark_suites:
  SimpleLambBenchmarks:
    performance_reader: StatsPerformance
    command: -S -E -N 3 -w %(input)s -s %(variable)s %(benchmark)s
    input_sizes: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,
                  18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,
                  33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,
                  48, 49, 50, 51, 52, 53]
    benchmarks:
      - reverse:
          extra_args: '"15000;i:1,f:+"'
      - append:
         extra_args: '"20000;i:1,f:+" "20000;i:1000000,f:-"'
      - plus:
          extra_args: 1000 1000
      - plusa:
          extra_args: 1000 1000
      - mult:
          extra_args: 100 100
      - multa:
          extra_args: 100 100
      - succ:
          extra_args: 10000
      - pred:
          extra_args: 10000
      # - map:
      #     extra_args: '"succ" "10000;p:1,f:succ"'
   variable_values: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
                      17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,
                      31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,
                      45, 46, 47, 48, 49, 50, 51, 52, 53]

# VMs have a name and are specified by a path and the binary to be executed
# optional: the number of cores for which the runs have to be executed
virtual_machines:
  lamb-c:
    path: .
    binary: lamb-c 
  lambi-c:
    path: .
    binary: lambi-c 
  lamb-c-nonil:
    path: .
    binary: lamb-c -n 


# define the benchmarks to be executed for a re-executable benchmark run
# special definitions done here should override benchmark suite definitions,
# and VM definitions
run_definitions:
  Lamb:
    description: >
      Default tests for lamb
    actions: benchmark
    benchmark:
      - SimpleLambBenchmarks
    #   - LambStatistics
    executions:
      - lamb-c
      - lambi-c
    reporting:
      csv_file: results/all.result.csv
    #  csv_raw:  results/all.data.csv
    # Test:
    #     description: >
    #         This run definition is used for testing.
    #         It should try all possible settings and the generated out
    #         will be compared to the expected one by the unit test(s)
    #     actions: profile
    #     benchmark:
    #         - TestSuite1
    #         - TestSuite2
    #     executions:
    #         # List of VMs and Benchmarks/Benchmark Suites to be run on them
    #         # benchmarks define here will override the ones defined for the
    #         # whole run the following example is equivalent to the global run
    #         # definition, but needs to be tested...
    #         - TestRunner1:
    #             benchmark: TestSuite1
    #         - TestRunner1:
    #             benchmark: TestSuite2
    #         - TestRunner2
    # TestProfiling:
    #     description: >
    #         This run is used to test the profiling run type
    #     actions: benchmark
    #     benchmark: TestSuite1
    #     input_sizes: 1
    #     executions:
    #         - CSOM
